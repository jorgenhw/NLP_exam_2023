{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from danlp.models import load_bert_tone_model\n",
    "import pandas as pd\n",
    "\n",
    "classifier = load_bert_tone_model()\n",
    "\n",
    "df = pd.read_csv('../data/twitter_data_paraphrasings_cleaned.csv', index_col=False)\n",
    "\n",
    "# choose a subset of the data\n",
    "#df = df.sample(n=10, random_state=1)\n",
    "\n",
    "# add columns with probabilities\n",
    "df['probabilities_text'] = df['text'].apply(lambda x: classifier.predict_proba(x, analytic=False)[0])\n",
    "df[['prob_pos_org', 'prob_neu_org', 'prob_neg_org']] = pd.DataFrame(df.probabilities_text.tolist(), index= df.index)\n",
    "df['probabilities_para'] = df['text_paraphrase_clean'].apply(lambda x: classifier.predict_proba(x, analytic=False)[0])\n",
    "df[['prob_pos_new', 'prob_neu_new', 'prob_neg_new']] = pd.DataFrame(df.probabilities_para.tolist(), index= df.index)\n",
    "#classifier._classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram of columns prob_pos_org, prob_neu_org, prob_neg_org, prob_pos_new, prob_neu_new, prob_neg_new\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# figure with 3 subplots\n",
    "fig, axs = plt.subplots(3, 2, sharex=True, sharey=True)\n",
    "axs[0, 0].hist(df['prob_pos_org'], bins=100)\n",
    "axs[0, 0].set_title('original')\n",
    "axs[0, 1].hist(df['prob_pos_new'], bins=100)\n",
    "axs[0, 1].set_title('paraphrase')\n",
    "axs[1, 0].hist(df['prob_neu_org'], bins=100)\n",
    "axs[1, 1].hist(df['prob_neu_new'], bins=100)\n",
    "axs[2, 0].hist(df['prob_neg_org'], bins=100)\n",
    "axs[2, 1].hist(df['prob_neg_new'], bins=100)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the difference between the probabilities\n",
    "df['diff_pos'] = df['prob_pos_new'] - df['prob_pos_org']\n",
    "df['diff_neu'] = df['prob_neu_new'] - df['prob_neu_org']\n",
    "df['diff_neg'] = df['prob_neg_new'] - df['prob_neg_org']\n",
    "\n",
    "# figure with 3 subplots\n",
    "fig, axs = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "axs[0].hist(df['diff_pos'], bins=100)\n",
    "axs[0].set_title('pos')\n",
    "axs[1].hist(df['diff_neu'], bins=100)\n",
    "axs[1].set_title('neu')\n",
    "axs[2].hist(df['diff_neg'], bins=100)\n",
    "axs[2].set_title('neg')\n",
    "# line at -0.3 and 0.3\n",
    "axs[0].axvline(x=-0.3, color='r', alpha=0.5)\n",
    "axs[0].axvline(x=0.3, color='r', alpha=0.5)\n",
    "axs[1].axvline(x=-0.3, color='r', alpha=0.5)\n",
    "axs[1].axvline(x=0.3, color='r', alpha=0.5)\n",
    "axs[2].axvline(x=-0.3, color='r', alpha=0.5)\n",
    "axs[2].axvline(x=0.3, color='r', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos 301\n",
      "pos 1147\n",
      "neu 396\n",
      "neu 1052\n",
      "neg 364\n",
      "neg 1084\n"
     ]
    }
   ],
   "source": [
    "# size of difference above absolute 0.3\n",
    "print('pos', len(df[df['diff_pos'].abs() > 0.3]))\n",
    "print('pos', len(df[df['diff_pos'].abs() < 0.3]))\n",
    "print('neu', len(df[df['diff_neu'].abs() > 0.3]))\n",
    "print('neu', len(df[df['diff_neu'].abs() < 0.3]))\n",
    "print('neg', len(df[df['diff_neg'].abs() > 0.3]))\n",
    "print('neg', len(df[df['diff_neg'].abs() < 0.3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% correct for org data:  correct_org\n",
      "True     0.700276\n",
      "False    0.299724\n",
      "Name: count, dtype: float64\n",
      "% correct for new data:  correct_new\n",
      "True     0.602901\n",
      "False    0.397099\n",
      "Name: count, dtype: float64\n",
      "% correct for filtered data org:  correct_org\n",
      "True     0.722459\n",
      "False    0.277541\n",
      "Name: count, dtype: float64\n",
      "% correct for filtered data new:  correct_new\n",
      "True     0.622545\n",
      "False    0.377455\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# create a new column with the predicted tone_new\n",
    "df['tone_new'] = df[['prob_pos_new', 'prob_neu_new', 'prob_neg_new']].idxmax(axis=1)\n",
    "df['tone_new'] = df['tone_new'].apply(lambda x: x.split('_')[1])\n",
    "df['tone_org'] = df[['prob_pos_org', 'prob_neu_org', 'prob_neg_org']].idxmax(axis=1)\n",
    "df['tone_org'] = df['tone_org'].apply(lambda x: x.split('_')[1])\n",
    "# print(df['tone_new'].value_counts())\n",
    "# print(df['tone_org'].value_counts())\n",
    "# print(df['label'].value_counts())\n",
    "\n",
    "# change tone to 'positive' and 'negative' and 'neutral'\n",
    "df['tone_new'] = df['tone_new'].apply(lambda x: 'positive' if x == 'pos' else x)\n",
    "df['tone_new'] = df['tone_new'].apply(lambda x: 'negative' if x == 'neg' else x)\n",
    "df['tone_new'] = df['tone_new'].apply(lambda x: 'neutral' if x == 'neu' else x)\n",
    "\n",
    "df['tone_org'] = df['tone_org'].apply(lambda x: 'positive' if x == 'pos' else x)\n",
    "df['tone_org'] = df['tone_org'].apply(lambda x: 'negative' if x == 'neg' else x)\n",
    "df['tone_org'] = df['tone_org'].apply(lambda x: 'neutral' if x == 'neu' else x)\n",
    "\n",
    "# compare predicted with label\n",
    "df['correct_org'] = df['tone_org'] == df['label']\n",
    "print('% correct for org data: ', df['correct_org'].value_counts()/len(df))\n",
    "df['correct_new'] = df['tone_new'] == df['label']\n",
    "print('% correct for new data: ', df['correct_new'].value_counts()/len(df))\n",
    "\n",
    "# filter out semantic semilarity below 0.7 and above 0.95 in a new dataframe\n",
    "df_filtered = df[(df['semantic_similarity'] > 0.7) & (df['semantic_similarity'] < 0.95)].copy()\n",
    "# compare predicted with label\n",
    "df_filtered['correct_org'] = df_filtered['tone_org'] == df_filtered['label']\n",
    "print('% correct for filtered data org: ', df_filtered['correct_org'].value_counts()/len(df_filtered))\n",
    "df_filtered['correct_new'] = df_filtered['tone_new'] == df_filtered['label']\n",
    "print('% correct for filtered data new: ', df_filtered['correct_new'].value_counts()/len(df_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# difference betweeen highest propability in org and the corresponding probability in new\n",
    "df_filtered['probabilities_para_argmax'] = df_filtered['probabilities_text'].apply(lambda x: x.argmax())\n",
    "df_filtered['selected_probability'] = df_filtered.apply(lambda row: row['probabilities_para'][int(row['probabilities_para_argmax'])], axis=1)\n",
    "df_filtered['diff_org_new'] = df_filtered['probabilities_text'].apply(lambda x: max(x)) - df_filtered['selected_probability']\n",
    "\n",
    "\n",
    "# difference betweeen highest propability in org and the corresponding probability in new\n",
    "df['probabilities_para_argmax'] = df['probabilities_text'].apply(lambda x: x.argmax())\n",
    "df['selected_probability'] = df.apply(lambda row: row['probabilities_para'][int(row['probabilities_para_argmax'])], axis=1)\n",
    "df['diff_org_new'] = df['probabilities_text'].apply(lambda x: max(x)) - df['selected_probability']\n",
    "\n",
    "# make two histograms of the differences\n",
    "# figure with 3 subplots\n",
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "axs[0].hist(df_filtered['diff_org_new'], bins=100)\n",
    "axs[0].set_title('filtered')\n",
    "axs[1].hist(df['diff_org_new'], bins=100)\n",
    "axs[1].set_title('all')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out diff_org_new below -0.3 and above 0.3 in a new dataframe\n",
    "df_filtered_sent = df_filtered[(df_filtered['diff_org_new'] > -0.3) & (df_filtered['diff_org_new'] < 0.3)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using librari textdescriptives to calculate readability and coherence\n",
    "# import textdescriptives as td\n",
    "\n",
    "# # will automatically download the relevant model (´en_core_web_lg´) and extract all metrics\n",
    "# df_text_qua = td.extract_metrics(text=df['text_paraphrase_clean'], lang=\"da\", metrics=[\"quality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156286f55528472285fd9ba997c40860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154a2075bed246bf970aaa4d621655fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9057586c24946489412f7410f189971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/255k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987e125b4cc24dbc8339ded7275c0fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27617d05c55c4f2d8f93442ff4e55eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88ba0833c834b92a42984270bd7283f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "import dacy\n",
    "\n",
    "nlp = spacy.blank(\"da\")  # an empty spacy pipeline\n",
    "# could also be a dacy pipeline, e.g. nlp = dacy.load(\"large\")\n",
    "nlp.add_pipe(\"dacy/polarity\")\n",
    "docs = nlp.pipe(df['text_paraphrase_clean'])\n",
    "\n",
    "nlp = spacy.blank(\"da\")  # an empty spacy pipeline\n",
    "# could also be a dacy pipeline, e.g. nlp = dacy.load(\"large\")\n",
    "nlp.add_pipe(\"dacy/polarity\")\n",
    "docs = nlp.pipe(df['text_paraphrase_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"da\")  # an empty spacy pipeline\n",
    "# could also be a dacy pipeline, e.g. nlp = dacy.load(\"large\")\n",
    "nlp.add_pipe(\"dacy/polarity\")\n",
    "docs = list(nlp.pipe(df['text_paraphrase_clean'][0:10]))\n",
    "\n",
    "prob_list = []\n",
    "\n",
    "for doc in docs:\n",
    "    # print the model predictions\n",
    "    prob_list.append(doc._.polarity_prob['prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(df['text_paraphrase_clean']))\n",
    "\n",
    "prob_list = []\n",
    "\n",
    "for doc in docs:\n",
    "    # print the model predictions\n",
    "    prob_list.append(doc._.polarity_prob['prob'])\n",
    "\n",
    "df['polarity_prob'] = prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nielsaalundkrogsgaard/documents_local/NLP_exam/NLP_exam_2023/niels_nbs/paraphrase_sentiment_validation.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nielsaalundkrogsgaard/documents_local/NLP_exam/NLP_exam_2023/niels_nbs/paraphrase_sentiment_validation.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mprobabilities_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39mlist\u001b[39;49m(nlp\u001b[39m.\u001b[39;49mpipe(x))[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m_\u001b[39m.\u001b[39;49mpolarity_prob[\u001b[39m'\u001b[39;49m\u001b[39mprob\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/pandas/core/series.py:4753\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4753\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4754\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   4755\u001b[0m         func,\n\u001b[1;32m   4756\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[1;32m   4757\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[1;32m   4758\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   4759\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m   4760\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[1;32m   1288\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[1;32m   1289\u001b[0m )\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2920\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/nielsaalundkrogsgaard/documents_local/NLP_exam/NLP_exam_2023/niels_nbs/paraphrase_sentiment_validation.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nielsaalundkrogsgaard/documents_local/NLP_exam/NLP_exam_2023/niels_nbs/paraphrase_sentiment_validation.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mprobabilities_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlist\u001b[39;49m(nlp\u001b[39m.\u001b[39;49mpipe(x))[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39mpolarity_prob[\u001b[39m'\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/spacy/language.py:1618\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[39mfor\u001b[39;00m pipe \u001b[39min\u001b[39;00m pipes:\n\u001b[1;32m   1617\u001b[0m         docs \u001b[39m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1618\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n\u001b[1;32m   1619\u001b[0m     \u001b[39myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/spacy/util.py:1685\u001b[0m, in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, name, default_error_handler, kwargs)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pipe\u001b[39m(\n\u001b[1;32m   1676\u001b[0m     docs: Iterable[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1677\u001b[0m     proc: \u001b[39m\"\u001b[39m\u001b[39mPipeCallable\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     kwargs: Mapping[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m   1683\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39m\"\u001b[39m\u001b[39mDoc\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m   1684\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(proc, \u001b[39m\"\u001b[39m\u001b[39mpipe\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1685\u001b[0m         \u001b[39myield from\u001b[39;00m proc\u001b[39m.\u001b[39mpipe(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1686\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1687\u001b[0m         \u001b[39m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m         kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/spacy_wrap/pipeline_component_seq_clf.py:217\u001b[0m, in \u001b[0;36mSequenceClassificationTransformer.pipe\u001b[0;34m(self, stream, batch_size)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mfor\u001b[39;00m indices \u001b[39min\u001b[39;00m batch_by_length(outer_batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg[\u001b[39m\"\u001b[39m\u001b[39mmax_batch_items\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m    216\u001b[0m     subbatch \u001b[39m=\u001b[39m [outer_batch[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices]\n\u001b[0;32m--> 217\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_annotations(subbatch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(subbatch))\n\u001b[1;32m    218\u001b[0m \u001b[39myield from\u001b[39;00m outer_batch\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/spacy_wrap/pipeline_component_seq_clf.py:234\u001b[0m, in \u001b[0;36mSequenceClassificationTransformer.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     activations \u001b[39m=\u001b[39m FullTransformerBatch\u001b[39m.\u001b[39mempty(\u001b[39mlen\u001b[39m(docs))  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     activations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(docs)\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m activations\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/spacy_transformers/layers/transformer_model.py:199\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    193\u001b[0m     align \u001b[39m=\u001b[39m get_alignment(\n\u001b[1;32m    194\u001b[0m         flat_spans, wordpieces\u001b[39m.\u001b[39mstrings, tokenizer\u001b[39m.\u001b[39mall_special_tokens\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m wordpieces, align \u001b[39m=\u001b[39m truncate_oversize_splits(\n\u001b[1;32m    197\u001b[0m     wordpieces, align, tokenizer\u001b[39m.\u001b[39mmodel_max_length\n\u001b[1;32m    198\u001b[0m )\n\u001b[0;32m--> 199\u001b[0m model_output, bp_tensors \u001b[39m=\u001b[39m transformer(wordpieces, is_train)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlogger\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mattrs:\n\u001b[1;32m    201\u001b[0m     log_gpu_memory(model\u001b[39m.\u001b[39mattrs[\u001b[39m\"\u001b[39m\u001b[39mlogger\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mafter forward\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/thinc/layers/pytorchwrapper.py:225\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m    222\u001b[0m convert_outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mattrs[\u001b[39m\"\u001b[39m\u001b[39mconvert_outputs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    224\u001b[0m Xtorch, get_dX \u001b[39m=\u001b[39m convert_inputs(model, X, is_train)\n\u001b[0;32m--> 225\u001b[0m Ytorch, torch_backprop \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mshims[\u001b[39m0\u001b[39;49m](Xtorch, is_train)\n\u001b[1;32m    226\u001b[0m Y, get_dYtorch \u001b[39m=\u001b[39m convert_outputs(model, (X, Ytorch), is_train)\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/thinc/shims/pytorch.py:97\u001b[0m, in \u001b[0;36mPyTorchShim.__call__\u001b[0;34m(self, inputs, is_train)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbegin_update(inputs)\n\u001b[1;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(inputs), \u001b[39mlambda\u001b[39;00m a: \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/thinc/shims/pytorch.py:115\u001b[0m, in \u001b[0;36mPyTorchShim.predict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mixed_precision):\n\u001b[0;32m--> 115\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(\u001b[39m*\u001b[39;49minputs\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    117\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1565\u001b[0m     input_ids,\n\u001b[1;32m   1566\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1567\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1568\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1569\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1570\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1574\u001b[0m )\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1016\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1017\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1018\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1019\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1020\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1021\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1022\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m         hidden_states,\n\u001b[1;32m    609\u001b[0m         attention_mask,\n\u001b[1;32m    610\u001b[0m         layer_head_mask,\n\u001b[1;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    613\u001b[0m         past_key_value,\n\u001b[1;32m    614\u001b[0m         output_attentions,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    541\u001b[0m )\n\u001b[1;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/pytorch_utils.py:241\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/documents_local/NLP_exam/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['probabilities_text'] = df['text'].apply(lambda x: list(nlp.pipe(x))[0]._.polarity_prob['prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.   , 0.001, 0.998], dtype=float32),\n",
       " array([0.071, 0.929, 0.001], dtype=float32),\n",
       " array([0.588, 0.409, 0.002], dtype=float32),\n",
       " array([0.006, 0.989, 0.006], dtype=float32),\n",
       " array([0.001, 0.998, 0.001], dtype=float32),\n",
       " array([0.   , 0.001, 0.998], dtype=float32),\n",
       " array([0.326, 0.671, 0.003], dtype=float32),\n",
       " array([0.016, 0.983, 0.001], dtype=float32),\n",
       " array([0.003, 0.996, 0.001], dtype=float32),\n",
       " array([0.001, 0.001, 0.999], dtype=float32)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add columns with probabilities\n",
    "df['probabilities_text'] = df['text'].apply(lambda x: nlp.predict_proba(x, analytic=False)[0])\n",
    "df[['prob_pos_org', 'prob_neu_org', 'prob_neg_org']] = pd.DataFrame(df.probabilities_text.tolist(), index= df.index)\n",
    "df['probabilities_para'] = df['text_paraphrase_clean'].apply(lambda x: classifier.predict_proba(x, analytic=False)[0])\n",
    "df[['prob_pos_new', 'prob_neu_new', 'prob_neg_new']] = pd.DataFrame(df.probabilities_para.tolist(), index= df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_NLP_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
