{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things one can focus at or pipeline sketch:\n",
    "\n",
    "1. [Build] Creating a robust and easy-implementable paraphrasing pipeline.\n",
    "\n",
    "2. [Modify] Creating a way to balance unbalanced datasets using paraphrasing (involves detection of imbalance)\n",
    "\n",
    "3. [Test]: Challenge existing text data augmentation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# defining examples dataset\n",
    "original = [\n",
    "    \"Danmark kan ikke leve af gæld.\",\n",
    "    \"Efter et regeringsskifte vil den økonomiske politik bygge på et meget klart forsigtighedsprincip.\",\n",
    "    \"Vi vil oprette et agentur mod falske nyheder, der udarbejder modsvar og korrigerer falske nyhedshistorier i Danmark.\",\n",
    "    \"Lige netop sygeskatten kunne have været et udspil, som LO og DA kunne have brugt selv til at få forhandlingerne på gled igen - alt i alt et indgreb og en lovgivning, der er tydeligt præget af at være udarbejdet af politikere fra Overdanmark med funktionærverdenen som eneste erfaringsgrundlag og uden særlig meget viden om, hvor skoen trykker.\",\n",
    "    \"HK, har ønsket at få kønsopdelt lønstatistik.\",\n",
    "    \"Så jeg synes, det er ret svært, når man skal igennem de her ting, og jeg synes heller ikke, vi kan være bekendt, at noget, der er så vigtigt som forbrugernes sikkerhed, når det gælder medicin, skal puttes ind sammen med en hel masse andre ting.\",\n",
    "    \"Grænseoverskridende problemer må løses gennem et grænseoverskridende samarbejde.\",\n",
    "    \"Forskellen mellem virkningerne på beskæftigelse og arbejdsløshed ligger i den øgede uddannelsesindsats.\",\n",
    "    \"Det er sådan, at jeg lige har været inde at tjekke min ordførertale, så jeg ikke skulle få samme svada fra hr.\",\n",
    "    \"Det ene er at give borgerne mulighed for frit at kunne rejse rundt i Europa, så de f.eks. ikke skal stå i lange køer, når de skal på ferie nede sydpå.\",\n",
    "    \"Derfor har vi også i den seneste folketingsperiode givet forsvaret det substantielle løft, som vi lovede, at der ville komme.\",\n",
    "    \"Dansk Folkeparti har her fremsat et beslutningsforslag, der pålægger regeringen at arbejde for en ændring af reglerne for EF-Domstolens arbejde, og så vidt jeg ved, skal det drøftes den 13. marts.\",\n",
    "    \"Vi skal være bedre til at få nye idéer omsat til produktion.\",\n",
    "    \"Vi har selvfølgelig noteret os, at der ikke er flertal for vores forslag.\",\n",
    "    \"Ved at starte en forbudsprocedure med udgangspunkt i listen over uønskede stoffer og de hormonforstyrrende kategori 1-stoffer i kosmetik tages et vigtigt skridt i den rigtige retning.\",\n",
    "    \"Mænd og kvinder og i øvrigt alle andre grupper skal have lige muligheder for at udfylde deres personlige potentiale.\",\n",
    "    \"Undervejs har vi truffet hårde beslutninger.\",\n",
    "    \"Regeringen magtede ikke at fortsætte en politik, som kunne forbedre erhvervslivets forhold.\",\n",
    "    \"Derfor undrer det mig, at der er så meget tvivl, og at man kan tro, at vi vil tage 20 pct. af de unge mennesker ind i den her ordning.\",\n",
    "    \"Så det går da trods alt, også i forhold til tallene fra sidste år, den rigtige vej.\"\n",
    "]\n",
    "\n",
    "paraphrased = [\n",
    "    \"Danmark kan ikke overleve ved konstant at låne penge.\",\n",
    "    \"Sker der et skift i regeringen, vil den nye økonomiske politik grundlæggende være baseret på en omhyggelig forsigtighed.\",\n",
    "    \"Vi vil danne en organisation til bekæmpelse af fake news, som vil udvikle svar og rette op på urigtige nyheder i Danmark.\",\n",
    "    \"Sygeskatten kunne have været noget, som LO og DA kunne have brugt til selv at sætte gang i forhandlingerne igen - i sidste ende en politisk handling og lovgivning, der tydeligvis er skabt af politikere fra den bedre del af samfundet, kun baseret på erfaringer fra administrativt arbejde og med begrænset indsigt i de konkrete problemer.\",\n",
    "    \"HK har udtrykt ønske om statistikker over lønninger opdelt efter køn.\",\n",
    "    \"Det er meget svært at håndtere disse problemstillinger, og jeg finder det også uacceptabelt, at forbrugernes medicinsikkerhed skal blandes sammen med en række andre mindre relevante sager.\",\n",
    "    \"Problemer, der går på tværs af landegrænser, skal adresseres gennem internationalt samarbejde.\",\n",
    "    \"Forskellen i påvirkningen af beskæftigelse og arbejdsløshed skyldes den forstærkede indsats i uddannelse.\",\n",
    "    \"Jeg har lige sikret mig, at jeg har tjekket min taler for at undgå at blive mødt med den samme monolog fra hr. [person].\",\n",
    "    \"Det handler om at give borgerne frihed til at rejse rundt i Europa uden at opleve lange ventetider, for eksempel ved grænsekontroller på ferier i syden.\",\n",
    "    \"Derfor har vi i den seneste valgperiode givet militæret den betydelige forstærkning, vi lovede ville finde sted.\",\n",
    "    \"Dansk Folkeparti har stillet et forslag, som forpligter regeringen til at arbejde for ændrede regler vedrørende EU-Domstolens funktion, og dette skal efter planen diskuteres den 13. marts.\",\n",
    "    \"Vi skal blive bedre til at transformere nye ideer til faktisk produktion.\",\n",
    "    \"Vi har selvfølgelig bemærket, at der ikke er opbakning til vores forslag.\",\n",
    "    \"Det er et væsentligt skridt i den rigtige retning at indlede en forbudsproces baseret på listen over bannede stoffer og hormonforstyrrende stoffer i kategori 1 i kosmetiske produkter.\",\n",
    "    \"Mænd og kvinder, samt alle andre, skal have lige adgang til at realisere deres personlige potentiale.\",\n",
    "    \"Undervejs har vi truffet nogle vanskelige valg.\",\n",
    "    \"Regeringen havde ikke styrken til at fortsætte med en politik, der kunne forbedre vilkårene for erhvervslivet.\",\n",
    "    \"Derfor undrer det mig, at der er så stor usikkerhed, og at nogen tror, vi vil inkludere 20 % af de unge i denne ordning.\",\n",
    "    \"Så på trods af alt tyder det i forhold til sidste års tal på, at vi er på rette vej.\"\n",
    "]\n",
    "\n",
    "examples = pd.DataFrame({\"original\": original, \"paraphrased\": paraphrased})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# before running this, run the following in the terminal: \n",
    "#1# CT_METAL=1 pip install ctransformers --no-binary ctransformers\n",
    "#2# pip install huggingface-hub\n",
    "#3# huggingface-cli download TheBloke/OpenHermes-2.5-Mistral-7B-GGUF openhermes-2.5-mistral-7b.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "\n",
    "\n",
    "# playing with mistral\n",
    "#from data_load import load_loop, map_questions\n",
    "from ctransformers import AutoModelForCausalLM, AutoConfig\n",
    "from pathlib import Path\n",
    "\n",
    "def make_input_mistral(phrase: str) -> str:\n",
    "    system = f\"\"\"Du er en sprogmodel som forstår og taler kompetent dansk.\n",
    "    Du svarer kun på dansk.\n",
    "    Din opgave er at omformulere tekst ved at finde synonymer med samme betydning.\n",
    "    Her er nogle eksempler på omformuleringer:\n",
    "    {examples.original[0]} -> {examples.paraphrased[0]},\n",
    "    {examples.original[1]} -> {examples.paraphrased[1]},\n",
    "    {examples.original[2]} -> {examples.paraphrased[2]},\n",
    "    {examples.original[3]} -> {examples.paraphrased[3]},\n",
    "    {examples.original[4]} -> {examples.paraphrased[4]},\n",
    "    {examples.original[5]} -> {examples.paraphrased[5]},\n",
    "    Her er nogle regler du skal overholde:\n",
    "    Du må ikke gentage dig selv.\n",
    "    Du må ikke bruge samme sætning som i originalen.\n",
    "    Du skal bruge samme kontekst som i originalen.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    <|im_start|>system\n",
    "    {system}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {phrase}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def load_mistral(model_path):\n",
    "    #config = AutoConfig.from_pretrained(model_path)\n",
    "    # Explicitly set the max_seq_len\n",
    "    #config.max_seq_len = 4096\n",
    "    #config.max_answer_len= 1024\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        str(model_path), \n",
    "        model_type=\"mistral\",\n",
    "        gpu_layers=50,\n",
    "        temperature=0.8, # default is 0.8\n",
    "        top_p = 0.95,\n",
    "        top_k = 40,  # default is 0.95\n",
    "        max_new_tokens = 1000,\n",
    "        context_length = 6000)\n",
    "    \n",
    "    return model\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_dataframe(original, model): # original = list of strings, model = output from the load_mistral() function\n",
    "    data = []\n",
    "    for string in tqdm(original):\n",
    "        new = model(make_input_mistral(string))\n",
    "        data.append([string, new])\n",
    "    df = pd.DataFrame(data, columns=['Original', 'New'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [01:28<00:49,  5.51s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "max_new_tokens = 50\n",
    "\n",
    "model = load_mistral(os.path.join(\"model\", \"openhermes-2.5-mistral-7b.Q4_K_M.gguf\"))\n",
    "\n",
    "strings = [\"Partiet Venstre er et borgerligt parti.\",\n",
    "           \"Social demokraterne var i regering fra 2011 til 2015.\",\n",
    "           \"Radikale venstre synes at skatten var for høj.\"]\n",
    "\n",
    "#for string in tqdm(original):\n",
    "#    new = model(make_input_mistral(string))\n",
    "#    print(f'[Original] {string} || [New] {new}')\n",
    "\n",
    "\n",
    "df = generate_dataframe(original, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Paraphrasing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer_config.json: 100%|██████████| 1.61k/1.61k [00:00<00:00, 601kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 1.46MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:01<00:00, 1.50MB/s]\n",
      "added_tokens.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 96.4kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 560/560 [00:00<00:00, 1.21MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "config.json: 100%|██████████| 613/613 [00:00<00:00, 1.26MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 23.9k/23.9k [00:00<00:00, 14.3MB/s]\n",
      "model-00001-of-00003.safetensors:   7%|▋         | 346M/4.94G [02:51<38:01, 2.02MB/s]\n",
      "Downloading shards:   0%|          | 0/3 [02:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mberkeley-nest/Starling-LM-7B-alpha\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mberkeley-nest/Starling-LM-7B-alpha\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/modeling_utils.py:3128\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3125\u001b[0m \u001b[39m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m \u001b[39mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3127\u001b[0m     \u001b[39m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3128\u001b[0m     resolved_archive_file, sharded_metadata \u001b[39m=\u001b[39m get_checkpoint_shard_files(\n\u001b[1;32m   3129\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3130\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3131\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   3132\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   3133\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   3134\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   3135\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   3136\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   3137\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   3138\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   3139\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   3140\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   3141\u001b[0m     )\n\u001b[1;32m   3143\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3144\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3145\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(resolved_archive_file, \u001b[39mstr\u001b[39m)\n\u001b[1;32m   3146\u001b[0m     \u001b[39mand\u001b[39;00m resolved_archive_file\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.safetensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3147\u001b[0m ):\n\u001b[1;32m   3148\u001b[0m     \u001b[39mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/utils/hub.py:1052\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39mfor\u001b[39;00m shard_filename \u001b[39min\u001b[39;00m tqdm(shard_filenames, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading shards\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1050\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1051\u001b[0m         \u001b[39m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m         cached_filename \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   1053\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1054\u001b[0m             shard_filename,\n\u001b[1;32m   1055\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1056\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1057\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1058\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1059\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1060\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1061\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1062\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1063\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1064\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49m_commit_hash,\n\u001b[1;32m   1065\u001b[0m         )\n\u001b[1;32m   1066\u001b[0m     \u001b[39m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m     \u001b[39m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/transformers/utils/hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    428\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    431\u001b[0m         path_or_repo_id,\n\u001b[1;32m    432\u001b[0m         filename,\n\u001b[1;32m    433\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    434\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    435\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    436\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    437\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    438\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    439\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    440\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    441\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    442\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/huggingface_hub/file_download.py:1461\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1459\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1461\u001b[0m     http_get(\n\u001b[1;32m   1462\u001b[0m         url_to_download,\n\u001b[1;32m   1463\u001b[0m         temp_file,\n\u001b[1;32m   1464\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1465\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1466\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1467\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1468\u001b[0m     )\n\u001b[1;32m   1470\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1471\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, _nb_retries)\u001b[0m\n\u001b[1;32m    539\u001b[0m new_resume_size \u001b[39m=\u001b[39m resume_size\n\u001b[1;32m    540\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    542\u001b[0m         \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m             progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/urllib3/response.py:525\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     chunk_amt \u001b[39m=\u001b[39m max_chunk_amt\n\u001b[0;32m--> 525\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(chunk_amt)\n\u001b[1;32m    526\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    527\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"berkeley-nest/Starling-LM-7B-alpha\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"berkeley-nest/Starling-LM-7B-alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27.7\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# show version of openai\n",
    "print(openai.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"\n",
    "1.\tDanmark kan ikke leve af gæld.\n",
    "2.\tEfter et regeringsskifte vil den økonomiske politik bygge på et meget klart forsigtighedsprincip.\n",
    "3.\tVi vil oprette et agentur mod falske nyheder, der udarbejder modsvar og korrigerer falske nyhedshistorier i Danmark.\n",
    "4.\tLige netop sygeskatten kunne have været et udspil, som LO og DA kunne have brugt selv til at få forhandlingerne på gled igen - alt i alt et indgreb og en lovgivning, der er tydeligt præget af at være udarbejdet af politikere fra Overdanmark med funktionærverdenen som eneste erfaringsgrundlag og uden særlig meget viden om, hvor skoen trykker.\n",
    "5.\tHK har ønsket at få kønsopdelt lønstatistik.\n",
    "6.\tSå jeg synes, det er ret svært, når man skal igennem de her ting, og jeg synes heller ikke, vi kan være bekendt, at noget, der er så vigtigt som forbrugernes sikkerhed, når det gælder medicin, skal puttes ind sammen med en hel masse andre ting.\n",
    "7.\tGrænseoverskridende problemer må løses gennem et grænseoverskridende samarbejde.\n",
    "8.\tForskellen mellem virkningerne på beskæftigelse og arbejdsløshed ligger i den øgede uddannelsesindsats.\n",
    "9.\tDet er sådan, at jeg lige har været inde at tjekke min ordførertale, så jeg ikke skulle få samme svada fra hr.\n",
    "10.\tDet ene er at give borgerne mulighed for frit at kunne rejse rundt i Europa, så de f.eks. ikke skal stå i lange køer, når de skal på ferie nede sydpå.\n",
    "11.\tDerfor har vi også i den seneste folketingsperiode givet forsvaret det substantielle løft, som vi lovede, at der ville komme.\n",
    "12.\tDansk Folkeparti har her fremsat et beslutningsforslag, der pålægger regeringen at arbejde for en ændring af reglerne for EF-Domstolens arbejde, og så vidt jeg ved, skal det drøftes den 13. marts.\n",
    "13.\tVi skal være bedre til at få nye idéer omsat til produktion.\n",
    "14.\tVi har selvfølgelig noteret os, at der ikke er flertal for vores forslag.\n",
    "15.\tVed at starte en forbudsprocedure med udgangspunkt i listen over uønskede stoffer og de hormonforstyrrende kategori 1-stoffer i kosmetik tages et vigtigt skridt i den rigtige retning.\n",
    "16.\tMænd og kvinder og i øvrigt alle andre grupper skal have lige muligheder for at udfylde deres personlige potentiale.\n",
    "17.\tUndervejs har vi truffet hårde beslutninger.\n",
    "18.\tRegeringen magtede ikke at fortsætte en politik, som kunne forbedre erhvervslivets forhold.\n",
    "19.\tDerfor undrer det mig, at der er så meget tvivl, og at man kan tro, at vi vil tage 20 pct. af de unge mennesker ind i den her ordning.\n",
    "20.\tSå det går da trods alt, også i forhold til tallene fra sidste år, den rigtige vej.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "import openai\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Paraphrase the following sentence (in Danish): \"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Danmark kan ikke overleve ved at have gæld.\\n2. Efter et regeringsskifte vil den økonomiske politik være baseret på et klart forsigtighedsprincip.\\n3. Vi vil etablere en agentur mod falske nyheder, som vil udarbejde modsvar og rette falske nyhedshistorier i Danmark.\\n4. Netop sygeskat kunne have været et udspil, som LO og DA selv kunne have brugt til at få forhandlingerne på sporet igen - alt i alt en indgriben og lovgivning, som tydeligvis er udviklet af politikere fra Overdanmark med kun funktionsverdenen som deres erfaringsgrundlag og uden særlig meget viden om, hvor problemet ligger.\\n5. HK har ønsket at få opdelt lønstatistik efter køn.\\n6. Så jeg synes, det er ret svært, når man skal igennem alt dette, og jeg synes heller ikke, at det er forsvarligt, at noget, der er så vigtigt som forbrugernes sikkerhed vedrørende medicin, bliver bundtet sammen med en masse andre ting.\\n7. Problemer på tværs af grænser skal løses gennem et internationalt samarbejde.\\n8. Forskellen mellem effekten på beskæftigelse og arbejdsløshed ligger i den øgede fokus på uddannelse.\\n9. Sådan er det, lige efter jeg har tjekket min ordførertale, så jeg ikke får samme nedsmeltning fra hr.\\n10. Det ene er at give borgerne mulighed for frit at rejse rundt i Europa, så de ikke skal stå i lange køer, når de skal på ferie sydpå.\\n11. Derfor har vi i den seneste folketingssession også givet forsvaret det betydelige løft, vi lovede.\\n12. Dansk Folkeparti har fremsat et beslutningsforslag her, der pålægger regeringen at arbejde for en ændring af EF-Domstolens funktion, og så vidt jeg ved, skal det drøftes den 13. marts.\\n13. Vi skal blive bedre til at omsætte nye idéer til produktion.\\n14. Vi har naturligvis noteret os, at vores forslag ikke har flertal.\\n15. Ved at igangsætte en forbudsprocedure baseret på listen over uønskede stoffer og kategori 1 hormonforstyrrende stoffer i kosmetik, tager vi et vigtigt skridt i den rigtige retning.\\n16. Mænd og kvinder, og i virkeligheden alle andre grupper, skal have lige muligheder for at opfylde deres personlige potentiale.\\n17. Undervejs har vi truffet svære beslutninger.\\n18. Regeringen var ikke i stand til at fortsætte en politik, der kunne forbedre forholdene for erhvervslivet.\\n19. Derfor undrer det mig, at der er så meget tvivl, og at man kan tro, at vi vil inkludere 20 pct. af de unge i denne ordning.\\n20. Så det går da fremad, også i forhold til tallene fra sidste år.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show completed chat\n",
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.987843]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"This is example sentence\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "#print(embeddings)\n",
    "\n",
    "# print cosine-similarities betwen the sentences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([embeddings[0]], [embeddings[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/wibe/nltk_data'\n    - '/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/nltk_data'\n    - '/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/share/nltk_data'\n    - '/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m syntax_errors\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThe cat sat on the mat.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m syntax_errors \u001b[39m=\u001b[39m check_sentence_syntax(sentence)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(syntax_errors) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSyntax errors found:\u001b[39m\u001b[39m\"\u001b[39m, syntax_errors)\n",
      "\u001b[1;32m/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Perform part-of-speech tagging on the words\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tagged_words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mpos_tag(words)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Check for any syntax errors\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/sandbox.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m syntax_errors \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/nltk/tag/__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/nltk/tag/__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    105\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[1;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/nltk/tag/perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[1;32m    166\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[0;32m--> 167\u001b[0m         find(\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m PICKLE)\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/wibe/nltk_data'\n    - '/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/nltk_data'\n    - '/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/share/nltk_data'\n    - '/Users/wibe/Desktop/CogSci/NLP/Exam_project/NLP_exam_2023/venv_NLP_exam/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def check_sentence_syntax(sentence):\n",
    "    # Tokenize the sentence into individual words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Perform part-of-speech tagging on the words\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "    # Check for any syntax errors\n",
    "    syntax_errors = []\n",
    "    for word, tag in tagged_words:\n",
    "        if tag.startswith('NN') and word.lower() != 'syntax':\n",
    "            syntax_errors.append(word)\n",
    "\n",
    "    return syntax_errors\n",
    "\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "syntax_errors = check_sentence_syntax(sentence)\n",
    "if len(syntax_errors) > 0:\n",
    "    print(\"Syntax errors found:\", syntax_errors)\n",
    "else:\n",
    "    print(\"No syntax errors found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
