{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data path\n",
    "path = os.path.join(\"..\" , \"data\" , \"moral_DA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_prepare_dataset(file_path):\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file_path)\n",
    "    dataset = dataset[:10]\n",
    "    #print the number of times a label is positive, negative or neutral\n",
    "    print(f\"Number of labels: { dataset['label'].value_counts()}\")\n",
    "\n",
    "    dataset = dataset.drop_duplicates()\n",
    "    # Convert to dict and then to a Hugging Face Dataset\n",
    "    dataset = Dataset.from_dict(dataset)\n",
    "    print(\"Dataset loaded and prepared\")\n",
    "    return dataset#, dataset_pd\n",
    "\n",
    "# Split the dataset and convert into a Hugging Face DatasetDict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def split_dataset(dataset, seed=42): #def split_dataset(dataset, path_to_df_train, path_to_df_train_2, seed=42):\n",
    "    \n",
    "    # load paraphrasings dataset\n",
    "    #paraphrasings = dataframe\n",
    "    # concatenate the paraphrasings dataset with the original dataset\n",
    "    #paraphrasings = pd.read_csv(path_to_df_train)\n",
    "    #paraphrasings2 = pd.read_csv(path_to_df_train_2)\n",
    "\n",
    "    #paraphrasings = pd.concat([paraphrasings, paraphrasings2])\n",
    "\n",
    "    # remove rows where org_or_new == 1 - removing original tweets\n",
    "    #paraphrasings = paraphrasings[paraphrasings['org_or_new'] == 0]\n",
    "    #print(\"Number of paraphrasings: \", len(paraphrasings))\n",
    "\n",
    "    #paraphrasings = paraphrasings.rename(columns={\"New\":\"text\"})\n",
    "    #paraphrasings = paraphrasings.drop(columns=[\"org_or_new\"])\n",
    "    #paraphrasings = paraphrasings[['paraphrased_text', 'label']]\n",
    "\n",
    "    # rename paraphrased_text to text\n",
    "    #paraphrasings = paraphrasings.rename(columns={\"paraphrased_text\":\"text\"})\n",
    "\n",
    "    # remove duplicates\n",
    "    #paraphrasings = paraphrasings.drop_duplicates()\n",
    "    #print(\"Number of paraphrasings after removing duplicates: \", len(paraphrasings))\n",
    "\n",
    "    #paraphrasings_plus_org = Dataset.from_dict(paraphrasings)\n",
    "\n",
    "\n",
    "    # 60% train, 20% validation, 20% test\n",
    "    train_test = dataset.train_test_split(test_size=0.4, seed=seed) \n",
    "    test_valid = train_test['test'].train_test_split(test_size=0.5, seed=seed)\n",
    "\n",
    "    # combine train, test and valid to one dictionary\n",
    "    dataset_splitted_dict = DatasetDict({\n",
    "        'train': train_test['train'],\n",
    "        'valid': test_valid['train'],\n",
    "        'test': test_valid['test']})\n",
    "    \n",
    "    # save the train dataset as a csv file\n",
    "    dataset_splitted_dict['train'].to_csv(os.path.join(\"..\", \"data\", \"train_MORALITY.csv\"))\n",
    "    \n",
    "    print(\"Dataset splitted into train (60%), valid (20%) and test (20%)\")\n",
    "\n",
    "    # output the train dataset as a csv file\n",
    "    #dataset_splitted_dict['train'].to_csv(os.path.join(\"..\", \"data\", \"train.csv\"))\n",
    "\n",
    "    # print the length of the train dataset\n",
    "    print(\"Length of train dataset: \", len(dataset_splitted_dict['train']))\n",
    "    print(\"Length of valid dataset: \", len(dataset_splitted_dict['valid']))\n",
    "    print(\"Length of test dataset: \", len(dataset_splitted_dict['test']))\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    return dataset_splitted_dict\n",
    "\n",
    "# Tokenize the dataset \n",
    "from transformers import AutoTokenizer\n",
    "from datasets import ClassLabel\n",
    "\n",
    "def tokenize_dataset(dataset, model_name=\"NbAiLab/nb-bert-large\", max_length=128): # max_length refers to the maximum length of the input text\n",
    "    # defining the labels\n",
    "    labels_cl = ClassLabel(num_classes=2, names=['0', '1'])\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # defining a function to tokenize the text and translate all labels into integers instead of strings\n",
    "    def tokenize_function(example):\n",
    "        tokens = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        tokens['label'] = labels_cl.str2int(example['label'])\n",
    "        return tokens\n",
    "\n",
    "    # actually tokenizing the dataset\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].column_names) # batched=True speeds up tokenization by allowing to process multiple lines at once\n",
    "\n",
    "\n",
    "    print(\"Dataset tokenized\")\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "# evaluation metrics\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric0 = evaluate.load(\"accuracy\")\n",
    "    metric1 = evaluate.load(\"precision\")\n",
    "    metric2 = evaluate.load(\"recall\")\n",
    "    metric3 = evaluate.load(\"f1\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric0.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "    f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing dataset...\n",
      "Number of labels: label\n",
      "1    6\n",
      "0    4\n",
      "Name: count, dtype: int64\n",
      "Dataset loaded and prepared\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Splitting dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 225.57ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splitted into train (60%), valid (20%) and test (20%)\n",
      "Length of train dataset:  6\n",
      "Length of valid dataset:  2\n",
      "Length of test dataset:  2\n",
      "\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 1616.41 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 555.83 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 540.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized\n",
      "Loading model (NbAiLab/nb-bert-large)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = load_and_prepare_dataset(path)\n",
    "print(dataset)\n",
    "\n",
    "print(\"Splitting dataset...\")\n",
    "dataset_splitted_dict = split_dataset(dataset) #split_dataset(dataset, path2, path3)\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = tokenize_dataset(dataset_splitted_dict)\n",
    "\n",
    "print(\"Loading model (NbAiLab/nb-bert-large)...\")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"NbAiLab/nb-bert-large\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['negative', 'neutral', 'positive'], dtype='<U8'), array([1764, 1422, 1100]))\n",
      "(array(['negative', 'neutral', 'positive'], dtype='<U8'), array([309, 221, 185]))\n",
      "(array(['negative', 'neutral', 'positive'], dtype='<U8'), array([268, 252, 194]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "# count number of labels in each dataset\n",
    "print(np.unique(dataset_splitted_dict['train']['label'],return_counts=True))\n",
    "print(np.unique(dataset_splitted_dict['test']['label'],return_counts=True))\n",
    "print(np.unique(dataset_splitted_dict['valid']['label'],return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 8 # stating batch size\n",
    "epochs = 4\n",
    "learning_rate = 1e-5\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  num_train_epochs=epochs,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  weight_decay=0.01,\n",
    "                                  logging_dir=\"logs\",\n",
    "                                  logging_steps=10,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",  # Add this line\n",
    "                                  remove_unused_columns=False,\n",
    "                                  run_name=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:26<00:00,  3.45it/s]\n",
      "100%|██████████| 90/90 [00:26<00:00,  3.37it/s]\n",
      "100%|██████████| 90/90 [00:26<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7034965034965035, 'precision': 0.7208567419348564, 'recall': 0.7034965034965035, 'f1': 0.7066208658676242}\n",
      "{'accuracy': 0.665266106442577, 'precision': 0.6759627052306597, 'recall': 0.665266106442577, 'f1': 0.6661282766130079}\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# creating model predictions for the validation data\n",
    "predictions_val = trainer.predict(tokenized_dataset[\"valid\"])\n",
    "\n",
    "# choosing the prediction that has the highest probability \n",
    "preds_val_val = np.argmax(predictions_val.predictions, axis=-1)\n",
    "\n",
    "# calculating the probabilities instead of logits from each\n",
    "predictions_probabilities = tf.nn.softmax(predictions_val.predictions)\n",
    "\n",
    "def compute_metrics_end(preds, refs):\n",
    "    metric0 = evaluate.load(\"accuracy\")\n",
    "    metric1 = evaluate.load(\"precision\")\n",
    "    metric2 = evaluate.load(\"recall\")\n",
    "    metric3 = evaluate.load(\"f1\")\n",
    "    \n",
    "    #logits, labels = eval_pred\n",
    "    #predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric0.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "    precision = metric1.compute(predictions=preds, references=refs, average=\"weighted\")[\"precision\"]\n",
    "    recall = metric2.compute(predictions=preds, references=refs, average=\"weighted\")[\"recall\"]\n",
    "    f1 = metric3.compute(predictions=preds, references=refs, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "metrics_val = compute_metrics_end(preds=preds_val_val, refs=predictions_val.label_ids)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# creating model predictions for the validation data\n",
    "predictions_test = trainer.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "# choosing the prediction that has the highest probability \n",
    "preds_test_test = np.argmax(predictions_test.predictions, axis=-1)\n",
    "\n",
    "# calculating the probabilities instead of logits from each\n",
    "predictions_probabilities_test = tf.nn.softmax(predictions_test.predictions)\n",
    "\n",
    "metrics_test = compute_metrics_end(preds=preds_test_test, refs=predictions_test.label_ids)\n",
    "\n",
    "print(metrics_test)\n",
    "print(metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Labels</th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Misclassification</th>\n",
       "      <th>Text</th>\n",
       "      <th>Logit Values</th>\n",
       "      <th>Probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Det er så ikke den, jeg var lidt for entusiast...</td>\n",
       "      <td>[ 0.9340587  -0.01499593 -1.1753395 ]</td>\n",
       "      <td>[0.66294634 0.25663105 0.08042265]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Åh gud... Har intet med det at gøre</td>\n",
       "      <td>[ 0.49472684  0.52091426 -1.059791  ]</td>\n",
       "      <td>[0.44686255 0.45871928 0.09441815]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Det kan du sige, men med Artikel 13 bliver det...</td>\n",
       "      <td>[ 0.61225605  0.7817579  -0.8384904 ]</td>\n",
       "      <td>[0.4133752  0.4897316  0.09689319]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Tak gode mand! Da jeg fik notifikationen, troe...</td>\n",
       "      <td>[-1.0158063 -0.6964614  2.1499164]</td>\n",
       "      <td>[0.03834047 0.05276516 0.90889436]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Den er særdeles troværdig. Viser sandheden om ...</td>\n",
       "      <td>[ 1.1367214  -0.47244602 -0.9655315 ]</td>\n",
       "      <td>[0.7562952  0.15129997 0.09240479]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Mere brug af GMO i landbruget kan ikke løse al...</td>\n",
       "      <td>[ 0.12570588  0.72970027 -1.148641  ]</td>\n",
       "      <td>[0.32164422 0.5884197  0.08993609]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Rusland fejrer femåret for annekteringen af Kr...</td>\n",
       "      <td>[-0.76456714  1.6279043  -0.502376  ]</td>\n",
       "      <td>[0.07552715 0.8263046  0.09816829]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Det er minimeret til det ekstreme, så måske ik...</td>\n",
       "      <td>[ 1.3416474 -0.5490633 -1.1396143]</td>\n",
       "      <td>[0.8099776  0.12227785 0.06774461]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>🇩🇰 #Superliga\\nGrupo 1 Descenso\\n29º Fecha\\n40...</td>\n",
       "      <td>[-1.4751852   1.7833322  -0.67671984]</td>\n",
       "      <td>[0.03420784 0.889778   0.07601419]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Så er der #klimakryds! Rigtig god valgkamp #dk...</td>\n",
       "      <td>[-1.406308    0.25475538  0.6032569 ]</td>\n",
       "      <td>[0.07285987 0.38360038 0.54353976]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Predicted Labels True Labels Misclassification  \\\n",
       "0           negative     neutral              MISS   \n",
       "1            neutral    negative              MISS   \n",
       "2            neutral    negative              MISS   \n",
       "3           positive    positive              TRUE   \n",
       "4           negative    negative              TRUE   \n",
       "..               ...         ...               ...   \n",
       "709          neutral    negative              MISS   \n",
       "710          neutral     neutral              TRUE   \n",
       "711         negative    negative              TRUE   \n",
       "712          neutral     neutral              TRUE   \n",
       "713         positive    positive              TRUE   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    Det er så ikke den, jeg var lidt for entusiast...   \n",
       "1                  Åh gud... Har intet med det at gøre   \n",
       "2    Det kan du sige, men med Artikel 13 bliver det...   \n",
       "3    Tak gode mand! Da jeg fik notifikationen, troe...   \n",
       "4    Den er særdeles troværdig. Viser sandheden om ...   \n",
       "..                                                 ...   \n",
       "709  Mere brug af GMO i landbruget kan ikke løse al...   \n",
       "710  Rusland fejrer femåret for annekteringen af Kr...   \n",
       "711  Det er minimeret til det ekstreme, så måske ik...   \n",
       "712  🇩🇰 #Superliga\\nGrupo 1 Descenso\\n29º Fecha\\n40...   \n",
       "713  Så er der #klimakryds! Rigtig god valgkamp #dk...   \n",
       "\n",
       "                              Logit Values                       Probabilities  \n",
       "0    [ 0.9340587  -0.01499593 -1.1753395 ]  [0.66294634 0.25663105 0.08042265]  \n",
       "1    [ 0.49472684  0.52091426 -1.059791  ]  [0.44686255 0.45871928 0.09441815]  \n",
       "2    [ 0.61225605  0.7817579  -0.8384904 ]  [0.4133752  0.4897316  0.09689319]  \n",
       "3       [-1.0158063 -0.6964614  2.1499164]  [0.03834047 0.05276516 0.90889436]  \n",
       "4    [ 1.1367214  -0.47244602 -0.9655315 ]  [0.7562952  0.15129997 0.09240479]  \n",
       "..                                     ...                                 ...  \n",
       "709  [ 0.12570588  0.72970027 -1.148641  ]  [0.32164422 0.5884197  0.08993609]  \n",
       "710  [-0.76456714  1.6279043  -0.502376  ]  [0.07552715 0.8263046  0.09816829]  \n",
       "711     [ 1.3416474 -0.5490633 -1.1396143]  [0.8099776  0.12227785 0.06774461]  \n",
       "712  [-1.4751852   1.7833322  -0.67671984]  [0.03420784 0.889778   0.07601419]  \n",
       "713  [-1.406308    0.25475538  0.6032569 ]  [0.07285987 0.38360038 0.54353976]  \n",
       "\n",
       "[714 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Predicted Labels': [\"negative\" if i == 0 else \"neutral\" if i == 1 else \"positive\" for i in preds_val_val],\n",
    "        'True Labels': [\"negative\" if i == 0 else \"neutral\" if i == 1 else \"positive\" for i in predictions_val.label_ids],\n",
    "        'Misclassification': [\"TRUE\" if preds_val_val[i] == predictions_val.label_ids[i] else 'MISS' for i, val in enumerate(preds_val_val)],\n",
    "        'Text': dataset_splitted_dict['valid']['text'],\n",
    "        'Logit Values': [str(i) for i in predictions_val.predictions],\n",
    "        'Probabilities': [str(i) for i in np.asarray(predictions_probabilities)]}\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.70      0.71       268\n",
      "     neutral       0.58      0.70      0.63       252\n",
      "    positive       0.73      0.58      0.65       194\n",
      "\n",
      "    accuracy                           0.67       714\n",
      "   macro avg       0.68      0.66      0.66       714\n",
      "weighted avg       0.68      0.67      0.67       714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Extract the true and predicted labels\n",
    "true_labels = df['True Labels']\n",
    "predicted_labels = df['Predicted Labels']\n",
    "\n",
    "# Create a mapping for the labels to numbers if needed\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "# Map the labels to numbers using the mapping\n",
    "true_labels_mapped = true_labels.map(label_mapping)\n",
    "predicted_labels_mapped = predicted_labels.map(label_mapping)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(true_labels_mapped, predicted_labels_mapped, target_names=label_mapping.keys())\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      "\u001b[A                                          \n",
      "\n",
      "  5%|▌         | 23/448 [02:17<07:04,  1.00it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.018054485321045, 'eval_accuracy': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 19.1606, 'eval_samples_per_second': 0.104, 'eval_steps_per_second': 0.052, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      "\u001b[A                                          \n",
      "\n",
      "  5%|▌         | 23/448 [04:33<07:04,  1.00it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9296965599060059, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 19.308, 'eval_samples_per_second': 0.104, 'eval_steps_per_second': 0.052, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      "\u001b[A                                           \n",
      "\n",
      "  5%|▌         | 23/448 [07:29<07:04,  1.00it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9607483744621277, 'eval_accuracy': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 23.8129, 'eval_samples_per_second': 0.084, 'eval_steps_per_second': 0.042, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                \n",
      "\u001b[A                                           \n",
      "\n",
      "  5%|▌         | 23/448 [10:08<07:04,  1.00it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0143837928771973, 'eval_accuracy': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 25.4005, 'eval_samples_per_second': 0.079, 'eval_steps_per_second': 0.039, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "100%|██████████| 4/4 [09:20<00:00, 140.09s/it]s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 560.3137, 'train_samples_per_second': 0.043, 'train_steps_per_second': 0.007, 'train_loss': 0.9341059923171997, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.08s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.49s/it]\n",
      "100%|██████████| 1/1 [00:07<00:00,  7.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n",
      "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/utils/_array_api.py:245: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y_true contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m predicted_labels_mapped \u001b[38;5;241m=\u001b[39m predicted_labels\u001b[38;5;241m.\u001b[39mmap(label_mapping)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Generate the classification report\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels_mapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels_mapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# save classification report to csv\u001b[39;00m\n\u001b[1;32m     80\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(report)\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2545\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2411\u001b[0m     {\n\u001b[1;32m   2412\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2436\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2437\u001b[0m ):\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[1;32m   2439\u001b[0m \n\u001b[1;32m   2440\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2545\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2548\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m---> 85\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/utils/multiclass.py:383\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    381\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m!=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(data, \u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m--> 383\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y_true contains NaN."
     ]
    }
   ],
   "source": [
    "for i in range(10):    \n",
    "    trainer.train()\n",
    "\n",
    "    trainer.evaluate()\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # creating model predictions for the validation data\n",
    "    predictions_val = trainer.predict(tokenized_dataset[\"valid\"])\n",
    "\n",
    "    # choosing the prediction that has the highest probability \n",
    "    preds_val_val = np.argmax(predictions_val.predictions, axis=-1)\n",
    "\n",
    "    # calculating the probabilities instead of logits from each\n",
    "    predictions_probabilities = tf.nn.softmax(predictions_val.predictions)\n",
    "\n",
    "    def compute_metrics_end(preds, refs):\n",
    "        metric0 = evaluate.load(\"accuracy\")\n",
    "        metric1 = evaluate.load(\"precision\")\n",
    "        metric2 = evaluate.load(\"recall\")\n",
    "        metric3 = evaluate.load(\"f1\")\n",
    "        \n",
    "        #logits, labels = eval_pred\n",
    "        #predictions = np.argmax(logits, axis=-1)\n",
    "        accuracy = metric0.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "        precision = metric1.compute(predictions=preds, references=refs, average=\"weighted\")[\"precision\"]\n",
    "        recall = metric2.compute(predictions=preds, references=refs, average=\"weighted\")[\"recall\"]\n",
    "        f1 = metric3.compute(predictions=preds, references=refs, average=\"weighted\")[\"f1\"]\n",
    "        return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    metrics_val = compute_metrics_end(preds=preds_val_val, refs=predictions_val.label_ids)\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # creating model predictions for the validation data\n",
    "    predictions_test = trainer.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "    # choosing the prediction that has the highest probability \n",
    "    preds_test_test = np.argmax(predictions_test.predictions, axis=-1)\n",
    "\n",
    "    # calculating the probabilities instead of logits from each\n",
    "    predictions_probabilities_test = tf.nn.softmax(predictions_test.predictions)\n",
    "\n",
    "    metrics_test = compute_metrics_end(preds=preds_test_test, refs=predictions_test.label_ids)\n",
    "\n",
    "    print(metrics_test)\n",
    "    print(metrics_val)\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    data = {'Predicted Labels': [1 if i == 2 else 0 for i in preds_val_val],\n",
    "        'True Labels': [1 if i == 2 else 0 for i in predictions_val.label_ids],\n",
    "        'Misclassification': [\"TRUE\" if preds_val_val[i] == predictions_val.label_ids[i] else 'MISS' for i, val in enumerate(preds_val_val)],\n",
    "        'Text': dataset_splitted_dict['valid']['text'],\n",
    "        'Logit Values': [str(i) for i in predictions_val.predictions],\n",
    "        'Probabilities': [str(i) for i in np.asarray(predictions_probabilities)]}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # Extract the true and predicted labels\n",
    "    true_labels = df['True Labels']\n",
    "    predicted_labels = df['Predicted Labels']\n",
    "\n",
    "    # Create a mapping for the labels to numbers if needed\n",
    "    label_mapping = {'no_morality': 0, 'morality': 1}\n",
    "\n",
    "    # Map the labels to numbers using the mapping\n",
    "    true_labels_mapped = true_labels.map(label_mapping)\n",
    "    predicted_labels_mapped = predicted_labels.map(label_mapping)\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(true_labels_mapped, predicted_labels_mapped, target_names=label_mapping.keys(), output_dict=True)\n",
    "\n",
    "\n",
    "    # save classification report to csv\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(f\"../classification_reports_morality/{i+1}classification_report_only_org_MORALITY_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_NLP_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
