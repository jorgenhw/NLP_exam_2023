{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"DDSC/europarl\")\n",
    "path1 = os.path.join(\"..\", \"data\", \"train_paraphrased_with_gpt4_HF_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Det er svært at sige, hvordan det frie valg vi...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeg synes stadig, det er lidt urimeligt, men d...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gennemsigtighed, klarhed, kontrol og eksempler...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dengang var Socialdemokraterne den største gru...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lissabontraktaten er et kompromisresultat mell...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>Der er ingen udelukkede muligheder, men det er...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>Jeg er ikke sikker på, om jeg er så imponeret ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>Det er mærkeligt, men det er udelukkende op ti...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>Den 24. november afholdt Kommissionen et møde ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>Jeg håber, at finansieringen til denne ordning...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1148 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text     label\n",
       "0    Det er svært at sige, hvordan det frie valg vi...   neutral\n",
       "1    Jeg synes stadig, det er lidt urimeligt, men d...  positive\n",
       "2    Gennemsigtighed, klarhed, kontrol og eksempler...   neutral\n",
       "3    Dengang var Socialdemokraterne den største gru...   neutral\n",
       "4    Lissabontraktaten er et kompromisresultat mell...   neutral\n",
       "..                                                 ...       ...\n",
       "569  Der er ingen udelukkede muligheder, men det er...   neutral\n",
       "570  Jeg er ikke sikker på, om jeg er så imponeret ...  negative\n",
       "571  Det er mærkeligt, men det er udelukkende op ti...   neutral\n",
       "572  Den 24. november afholdt Kommissionen et møde ...   neutral\n",
       "573  Jeg håber, at finansieringen til denne ordning...   neutral\n",
       "\n",
       "[1148 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(path1)\n",
    "\n",
    "df1 = pd.read_csv(path1)\n",
    "df2 = df1[[\"text\", \"label\"]]\n",
    "df3 = df1[[\"paraphrased_text\", \"label\"]]\n",
    "# rename paraphrased_text to text\n",
    "df3 = df3.rename(columns={\"paraphrased_text\": \"text\"})\n",
    "\n",
    "df4 = pd.concat([df2, df3])\n",
    "\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Næste punkt på dagsordenen er forhandling unde...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vi mener, at eftersom får og geder var omfatte...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I tillæg dertil er der stadig indblanding fra ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deres indflydelse på de beslutninger, der træf...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hvis De venligst kan bringe orden i det, fru f...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>Vi har nu arbejdet sammen med alle grupper og ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Derfor skal der være kollektiv finansiering på...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Derfor er det igen vigtigt for planlægning og ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>Jo mere EU forbedrer kvaliteten af uddannelse ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>Hr. formand, jeg vil gerne starte med at sige,...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>957 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text     label\n",
       "0    Næste punkt på dagsordenen er forhandling unde...   neutral\n",
       "1    Vi mener, at eftersom får og geder var omfatte...  positive\n",
       "2    I tillæg dertil er der stadig indblanding fra ...  negative\n",
       "3    Deres indflydelse på de beslutninger, der træf...   neutral\n",
       "4    Hvis De venligst kan bringe orden i det, fru f...   neutral\n",
       "..                                                 ...       ...\n",
       "952  Vi har nu arbejdet sammen med alle grupper og ...   neutral\n",
       "953  Derfor skal der være kollektiv finansiering på...   neutral\n",
       "954  Derfor er det igen vigtigt for planlægning og ...   neutral\n",
       "955  Jo mere EU forbedrer kvaliteten af uddannelse ...  positive\n",
       "956  Hr. formand, jeg vil gerne starte med at sige,...   neutral\n",
       "\n",
       "[957 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# dataset dictionary to pandas dataframe\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df1 = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "# concatenate train and test dataframes\n",
    "df_HF = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "# rename negativ to negative\n",
    "df_HF = df_HF.replace(to_replace=\"negativ\", value=\"negative\")\n",
    "# rename positiv to positive\n",
    "df_HF = df_HF.replace(to_replace=\"positiv\", value=\"positive\")\n",
    "\n",
    "df_HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_prepare_dataset(file):\n",
    "    # Load the dataset\n",
    "    dataset = file\n",
    "    # Convert to dict and then to a Hugging Face Dataset\n",
    "    dataset = Dataset.from_dict(dataset)\n",
    "    print(\"Dataset loaded and prepared\")\n",
    "    return dataset#, dataset_pd\n",
    "\n",
    "# Split the dataset and convert into a Hugging Face DatasetDict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def split_dataset(dataset, dataset_2, seed=42): #def split_dataset(dataset, path_to_df_train, path_to_df_train_2, seed=42):\n",
    "    \n",
    "    # load paraphrasings dataset\n",
    "    paraphrasings = dataset_2\n",
    "    # concatenate the paraphrasings dataset with the original dataset\n",
    "    #paraphrasings = pd.read_csv(path_to_df_train)\n",
    "    #paraphrasings2 = pd.read_csv(path_to_df_train_2)\n",
    "\n",
    "    #paraphrasings = pd.concat([paraphrasings, paraphrasings2])\n",
    "\n",
    "    # remove rows where org_or_new == 1 - removing original tweets\n",
    "    #paraphrasings = paraphrasings[paraphrasings['org_or_new'] == 0]\n",
    "    #print(\"Number of paraphrasings: \", len(paraphrasings))\n",
    "\n",
    "    #paraphrasings = paraphrasings.rename(columns={\"New\":\"text\"})\n",
    "    #paraphrasings = paraphrasings.drop(columns=[\"org_or_new\"])\n",
    "    #paraphrasings = paraphrasings[['paraphrased_text', 'label']]\n",
    "\n",
    "    # rename paraphrased_text to text\n",
    "    #paraphrasings = paraphrasings.rename(columns={\"paraphrased_text\":\"text\"})\n",
    "\n",
    "    # remove duplicates\n",
    "    #paraphrasings = paraphrasings.drop_duplicates()\n",
    "    #print(\"Number of paraphrasings after removing duplicates: \", len(paraphrasings))\n",
    "\n",
    "    paraphrasings_plus_org = Dataset.from_dict(paraphrasings)\n",
    "\n",
    "\n",
    "    # 60% train, 20% validation, 20% test\n",
    "    train_test = dataset.train_test_split(test_size=0.4, seed=seed) \n",
    "    test_valid = train_test['test'].train_test_split(test_size=0.5, seed=seed)\n",
    "\n",
    "    # combine train, test and valid to one dictionary\n",
    "    dataset_splitted_dict = DatasetDict({\n",
    "        'train': paraphrasings_plus_org,\n",
    "        'valid': test_valid['train'],\n",
    "        'test': test_valid['test']})\n",
    "    \n",
    "    print(\"Dataset splitted into train (60%), valid (20%) and test (20%)\")\n",
    "\n",
    "    # output the train dataset as a csv file\n",
    "    dataset_splitted_dict['train'].to_csv(os.path.join(\"..\", \"data\", \"train_HF_df.csv\"))\n",
    "\n",
    "    # print the length of the train dataset\n",
    "    print(\"Length of train dataset: \", len(dataset_splitted_dict['train']))\n",
    "    print(\"Length of valid dataset: \", len(dataset_splitted_dict['valid']))\n",
    "    print(\"Length of test dataset: \", len(dataset_splitted_dict['test']))\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    return dataset_splitted_dict\n",
    "\n",
    "# Tokenize the dataset \n",
    "from transformers import AutoTokenizer\n",
    "from datasets import ClassLabel\n",
    "\n",
    "def tokenize_dataset(dataset, model_name=\"NbAiLab/nb-bert-large\", max_length=128):\n",
    "    # defining the labels\n",
    "    labels_cl = ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'])\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # defining a function to tokenize the text and translate all labels into integers instead of strings\n",
    "    def tokenize_function(example):\n",
    "        tokens = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        tokens['label'] = labels_cl.str2int(example['label'])\n",
    "        return tokens\n",
    "\n",
    "    # actually tokenizing the dataset\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].column_names) # batched=True speeds up tokenization by allowing to process multiple lines at once\n",
    "\n",
    "\n",
    "    print(\"Dataset tokenized\")\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "# evaluation metrics\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric0 = evaluate.load(\"accuracy\")\n",
    "    metric1 = evaluate.load(\"precision\")\n",
    "    metric2 = evaluate.load(\"recall\")\n",
    "    metric3 = evaluate.load(\"f1\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric0.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "    f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing dataset...\n",
      "Dataset loaded and prepared\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 957\n",
      "})\n",
      "Splitting dataset...\n",
      "Dataset splitted into train (60%), valid (20%) and test (20%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating CSV from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 242.72ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset:  1148\n",
      "Length of valid dataset:  191\n",
      "Length of test dataset:  192\n",
      "\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Map: 100%|██████████| 1148/1148 [00:00<00:00, 22908.24 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 191/191 [00:00<00:00, 17414.34 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 192/192 [00:00<00:00, 16892.65 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized\n",
      "Loading model (NbAiLab/nb-bert-large)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = load_and_prepare_dataset(df_HF)\n",
    "print(dataset)\n",
    "\n",
    "print(\"Splitting dataset...\")\n",
    "dataset_splitted_dict = split_dataset(dataset, df4) #split_dataset(dataset, path2, path3)\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = tokenize_dataset(dataset_splitted_dict)\n",
    "\n",
    "print(\"Loading model (NbAiLab/nb-bert-large)...\")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"NbAiLab/nb-bert-large\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['negative', 'neutral', 'positive'], dtype='<U8'), array([268, 592, 288]))\n",
      "(array(['negative', 'neutral', 'positive'], dtype='<U8'), array([37, 96, 59]))\n",
      "(array(['negative', 'neutral', 'positive'], dtype='<U8'), array([ 44, 104,  43]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "# count number of labels in each dataset\n",
    "print(np.unique(dataset_splitted_dict['train']['label'],return_counts=True))\n",
    "print(np.unique(dataset_splitted_dict['test']['label'],return_counts=True))\n",
    "print(np.unique(dataset_splitted_dict['valid']['label'],return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 8 # stating batch size\n",
    "epochs = 4\n",
    "learning_rate = 1e-5\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  num_train_epochs=epochs,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  weight_decay=0.01,\n",
    "                                  logging_dir=\"logs\",\n",
    "                                  logging_steps=10,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",  # Add this line\n",
    "                                  remove_unused_columns=False,\n",
    "                                  run_name=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [2:53:19<00:00, 2599.86s/it]\n",
      "  2%|▏         | 10/576 [01:34<1:17:03,  8.17s/it]"
     ]
    }
   ],
   "source": [
    "for i in range(10):    \n",
    "    trainer.train()\n",
    "\n",
    "    trainer.evaluate()\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # creating model predictions for the validation data\n",
    "    predictions_val = trainer.predict(tokenized_dataset[\"valid\"])\n",
    "\n",
    "    # choosing the prediction that has the highest probability \n",
    "    preds_val_val = np.argmax(predictions_val.predictions, axis=-1)\n",
    "\n",
    "    # calculating the probabilities instead of logits from each\n",
    "    predictions_probabilities = tf.nn.softmax(predictions_val.predictions)\n",
    "\n",
    "    def compute_metrics_end(preds, refs):\n",
    "        metric0 = evaluate.load(\"accuracy\")\n",
    "        metric1 = evaluate.load(\"precision\")\n",
    "        metric2 = evaluate.load(\"recall\")\n",
    "        metric3 = evaluate.load(\"f1\")\n",
    "        \n",
    "        #logits, labels = eval_pred\n",
    "        #predictions = np.argmax(logits, axis=-1)\n",
    "        accuracy = metric0.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "        precision = metric1.compute(predictions=preds, references=refs, average=\"weighted\")[\"precision\"]\n",
    "        recall = metric2.compute(predictions=preds, references=refs, average=\"weighted\")[\"recall\"]\n",
    "        f1 = metric3.compute(predictions=preds, references=refs, average=\"weighted\")[\"f1\"]\n",
    "        return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    metrics_val = compute_metrics_end(preds=preds_val_val, refs=predictions_val.label_ids)\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # creating model predictions for the validation data\n",
    "    predictions_test = trainer.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "    # choosing the prediction that has the highest probability \n",
    "    preds_test_test = np.argmax(predictions_test.predictions, axis=-1)\n",
    "\n",
    "    # calculating the probabilities instead of logits from each\n",
    "    predictions_probabilities_test = tf.nn.softmax(predictions_test.predictions)\n",
    "\n",
    "    metrics_test = compute_metrics_end(preds=preds_test_test, refs=predictions_test.label_ids)\n",
    "\n",
    "    print(metrics_test)\n",
    "    print(metrics_val)\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    data = {'Predicted Labels': [\"negative\" if i == 0 else \"neutral\" if i == 1 else \"positive\" for i in preds_val_val],\n",
    "            'True Labels': [\"negative\" if i == 0 else \"neutral\" if i == 1 else \"positive\" for i in predictions_val.label_ids],\n",
    "            'Misclassification': [\"TRUE\" if preds_val_val[i] == predictions_val.label_ids[i] else 'MISS' for i, val in enumerate(preds_val_val)],\n",
    "            'Text': dataset_splitted_dict['valid']['text'],\n",
    "            'Logit Values': [str(i) for i in predictions_val.predictions],\n",
    "            'Probabilities': [str(i) for i in np.asarray(predictions_probabilities)]}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # Extract the true and predicted labels\n",
    "    true_labels = df['True Labels']\n",
    "    predicted_labels = df['Predicted Labels']\n",
    "\n",
    "    # Create a mapping for the labels to numbers if needed\n",
    "    label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "    # Map the labels to numbers using the mapping\n",
    "    true_labels_mapped = true_labels.map(label_mapping)\n",
    "    predicted_labels_mapped = predicted_labels.map(label_mapping)\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(true_labels_mapped, predicted_labels_mapped, target_names=label_mapping.keys(), output_dict=True)\n",
    "\n",
    "\n",
    "    # save classification report to csv\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(f\"../classification_reports_HF/{i+1}classification_report_1x_gpt4paraphrased_plus_org.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_NLP_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
