{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data path\n",
    "path = os.path.join(\"..\" , \"data\" , \"tweets_data_temp.csv\")\n",
    "path2 = os.path.join(\"..\" , \"data\" , \"paraphrasings_on_train_2148rows_seed42.csv\")\n",
    "path3 = os.path.join(\"..\" , \"data\" , \"paraphrasings_on_train_2148rows_seed42_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Øh, tak! Her er et link til et interview, der ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>det kan være værdt at nævne, at dyr også vari...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#3 Om ateismen: \"Ateister mener, at religione...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Øjeblikkeligt: Vild jubel over forøget politiu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Selvom vi ikke ved det præcise datum for næst...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>Hun er 12 og voksen på den måde, man kun kan v...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3313</th>\n",
       "      <td>Kalder alle Broncos-fans. \\n\\nDrew Lock har be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>Se masser af billeder fra det traditionelle ra...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>Mere #entrepreneurshipeducation og mere #godun...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>Det er lidt hyklerisk, at Ida ikke svarer kvin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3317 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     label\n",
       "0     Øh, tak! Her er et link til et interview, der ...  positive\n",
       "1      det kan være værdt at nævne, at dyr også vari...   neutral\n",
       "2      #3 Om ateismen: \"Ateister mener, at religione...  negative\n",
       "3     Øjeblikkeligt: Vild jubel over forøget politiu...  negative\n",
       "4      Selvom vi ikke ved det præcise datum for næst...  negative\n",
       "...                                                 ...       ...\n",
       "3312  Hun er 12 og voksen på den måde, man kun kan v...  negative\n",
       "3313  Kalder alle Broncos-fans. \\n\\nDrew Lock har be...  positive\n",
       "3314  Se masser af billeder fra det traditionelle ra...   neutral\n",
       "3315  Mere #entrepreneurshipeducation og mere #godun...  positive\n",
       "3316  Det er lidt hyklerisk, at Ida ikke svarer kvin...  negative\n",
       "\n",
       "[3317 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(path2).head()\n",
    "# rename New to text\n",
    "df = pd.read_csv(path2).rename(columns={\"New\":\"text\"})\n",
    "df\n",
    "# delete ord_or_new column\n",
    "df = df.drop(columns=[\"org_or_new\"])\n",
    "df\n",
    "df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_prepare_dataset(file_path):\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file_path)\n",
    "    #print the number of times a label is positive, negative or neutral\n",
    "    print(dataset['label'].value_counts())\n",
    "    # equalize dataset making sure there are the same number of positive, negative and neutral tweets\n",
    "    # Remove all rows where language is not 'da'\n",
    "    dataset = dataset[dataset['language'] == 'da']\n",
    "    # Remove all columns except 'text' and 'label'\n",
    "    dataset = dataset[['text', 'label']]\n",
    "    # Remove all duplicates\n",
    "    dataset = dataset.drop_duplicates()\n",
    "    # Convert to dict and then to a Hugging Face Dataset\n",
    "    dataset = Dataset.from_dict(dataset)\n",
    "    print(\"Dataset loaded and prepared\")\n",
    "    return dataset#, dataset_pd\n",
    "\n",
    "# Split the dataset and convert into a Hugging Face DatasetDict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def split_dataset(dataset,path_to_df_train, seed=42): #def split_dataset(dataset, path_to_df_train, path_to_df_train_2, seed=42):\n",
    "    \n",
    "    # concatenate the paraphrasings dataset with the original dataset\n",
    "    paraphrasings = pd.read_csv(path_to_df_train)\n",
    "    #paraphrasings2 = pd.read_csv(path_to_df_train_2)\n",
    "\n",
    "    #paraphrasings = pd.concat([paraphrasings, paraphrasings2])\n",
    "\n",
    "    # remove rows where org_or_new\n",
    "\n",
    "    # remove duplicates\n",
    "    #paraphrasings = paraphrasings.drop_duplicates()\n",
    "    paraphrasings = paraphrasings.rename(columns={\"New\":\"text\"})\n",
    "    paraphrasings = paraphrasings.drop(columns=[\"org_or_new\"])\n",
    "    paraphrasings = paraphrasings[['text', 'label']]\n",
    "    paraphrasings_plus_org = Dataset.from_dict(paraphrasings)\n",
    "\n",
    "\n",
    "    # 60% train, 20% validation, 20% test\n",
    "    train_test = dataset.train_test_split(test_size=0.4, seed=seed) \n",
    "    test_valid = train_test['test'].train_test_split(test_size=0.5, seed=seed)\n",
    "\n",
    "    # combine train, test and valid to one dictionary\n",
    "    dataset_splitted_dict = DatasetDict({\n",
    "        'train': paraphrasings_plus_org,\n",
    "        'valid': test_valid['train'],\n",
    "        'test': test_valid['test']})\n",
    "    \n",
    "    print(\"Dataset splitted into train (60%), valid (20%) and test (20%)\")\n",
    "\n",
    "    # output the train dataset as a csv file\n",
    "    #dataset_splitted_dict['train'].to_csv(os.path.join(\"..\", \"data\", \"train.csv\"))\n",
    "\n",
    "    # print the length of the train dataset\n",
    "    print(\"Length of train dataset: \", len(dataset_splitted_dict['train']))\n",
    "    print(\"Length of valid dataset: \", len(dataset_splitted_dict['valid']))\n",
    "    print(\"Length of test dataset: \", len(dataset_splitted_dict['test']))\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    return dataset_splitted_dict\n",
    "\n",
    "# Tokenize the dataset \n",
    "from transformers import AutoTokenizer\n",
    "from datasets import ClassLabel\n",
    "\n",
    "def tokenize_dataset(dataset, model_name=\"NbAiLab/nb-bert-large\", max_length=128):\n",
    "    # defining the labels\n",
    "    labels_cl = ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'])\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # defining a function to tokenize the text and translate all labels into integers instead of strings\n",
    "    def tokenize_function(example):\n",
    "        tokens = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        tokens['label'] = labels_cl.str2int(example['label'])\n",
    "        return tokens\n",
    "\n",
    "    # actually tokenizing the dataset\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].column_names) # batched=True speeds up tokenization by allowing to process multiple lines at once\n",
    "\n",
    "\n",
    "    print(\"Dataset tokenized\")\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "# evaluation metrics\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric0 = evaluate.load(\"accuracy\")\n",
    "    metric1 = evaluate.load(\"precision\")\n",
    "    metric2 = evaluate.load(\"recall\")\n",
    "    metric3 = evaluate.load(\"f1\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric0.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "    f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing dataset...\n",
      "label\n",
      "negative    1525\n",
      "neutral     1281\n",
      "positive    1000\n",
      "Name: count, dtype: int64\n",
      "Dataset loaded and prepared\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 3572\n",
      "})\n",
      "Splitting dataset...\n",
      "Dataset splitted into train (60%), valid (20%) and test (20%)\n",
      "Length of train dataset:  3317\n",
      "Length of valid dataset:  714\n",
      "Length of test dataset:  715\n",
      "\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3317/3317 [00:00<00:00, 18946.47 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 20967.26 examples/s]\n",
      "Map: 100%|██████████| 715/715 [00:00<00:00, 20884.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized\n",
      "Loading model (NbAiLab/nb-bert-large)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = load_and_prepare_dataset(path)\n",
    "print(dataset)\n",
    "\n",
    "print(\"Splitting dataset...\")\n",
    "dataset_splitted_dict = split_dataset(dataset, path2) #split_dataset(dataset, path2, path3)\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = tokenize_dataset(dataset_splitted_dict)\n",
    "\n",
    "print(\"Loading model (NbAiLab/nb-bert-large)...\")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"NbAiLab/nb-bert-large\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 4436\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 714\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 715\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_splitted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['negative', 'neutral', 'positive'], dtype='<U8'), array([1806, 1439, 1191]))\n",
      "(array(['negative', 'neutral', 'positive'], dtype='<U8'), array([309, 221, 185]))\n",
      "(array(['negative', 'neutral', 'positive'], dtype='<U8'), array([268, 252, 194]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "# count number of labels in each dataset\n",
    "print(np.unique(dataset_splitted_dict['train']['label'],return_counts=True))\n",
    "print(np.unique(dataset_splitted_dict['test']['label'],return_counts=True))\n",
    "print(np.unique(dataset_splitted_dict['valid']['label'],return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 8 # stating batch size\n",
    "epochs = 4\n",
    "learning_rate = 1e-5\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  num_train_epochs=epochs,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  weight_decay=0.01,\n",
    "                                  logging_dir=\"logs\",\n",
    "                                  logging_steps=10,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",  # Add this line\n",
    "                                  remove_unused_columns=False,\n",
    "                                  run_name=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:26<00:00,  3.45it/s]\n",
      "100%|██████████| 90/90 [00:26<00:00,  3.37it/s]\n",
      "100%|██████████| 90/90 [00:26<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7034965034965035, 'precision': 0.7208567419348564, 'recall': 0.7034965034965035, 'f1': 0.7066208658676242}\n",
      "{'accuracy': 0.665266106442577, 'precision': 0.6759627052306597, 'recall': 0.665266106442577, 'f1': 0.6661282766130079}\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# creating model predictions for the validation data\n",
    "predictions_val = trainer.predict(tokenized_dataset[\"valid\"])\n",
    "\n",
    "# choosing the prediction that has the highest probability \n",
    "preds_val_val = np.argmax(predictions_val.predictions, axis=-1)\n",
    "\n",
    "# calculating the probabilities instead of logits from each\n",
    "predictions_probabilities = tf.nn.softmax(predictions_val.predictions)\n",
    "\n",
    "def compute_metrics_end(preds, refs):\n",
    "    metric0 = evaluate.load(\"accuracy\")\n",
    "    metric1 = evaluate.load(\"precision\")\n",
    "    metric2 = evaluate.load(\"recall\")\n",
    "    metric3 = evaluate.load(\"f1\")\n",
    "    \n",
    "    #logits, labels = eval_pred\n",
    "    #predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric0.compute(predictions=preds, references=refs)[\"accuracy\"]\n",
    "    precision = metric1.compute(predictions=preds, references=refs, average=\"weighted\")[\"precision\"]\n",
    "    recall = metric2.compute(predictions=preds, references=refs, average=\"weighted\")[\"recall\"]\n",
    "    f1 = metric3.compute(predictions=preds, references=refs, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "metrics_val = compute_metrics_end(preds=preds_val_val, refs=predictions_val.label_ids)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# creating model predictions for the validation data\n",
    "predictions_test = trainer.predict(tokenized_dataset[\"test\"])\n",
    "\n",
    "# choosing the prediction that has the highest probability \n",
    "preds_test_test = np.argmax(predictions_test.predictions, axis=-1)\n",
    "\n",
    "# calculating the probabilities instead of logits from each\n",
    "predictions_probabilities_test = tf.nn.softmax(predictions_test.predictions)\n",
    "\n",
    "metrics_test = compute_metrics_end(preds=preds_test_test, refs=predictions_test.label_ids)\n",
    "\n",
    "print(metrics_test)\n",
    "print(metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Labels</th>\n",
       "      <th>True Labels</th>\n",
       "      <th>Misclassification</th>\n",
       "      <th>Text</th>\n",
       "      <th>Logit Values</th>\n",
       "      <th>Probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Det er så ikke den, jeg var lidt for entusiast...</td>\n",
       "      <td>[ 0.9340587  -0.01499593 -1.1753395 ]</td>\n",
       "      <td>[0.66294634 0.25663105 0.08042265]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Åh gud... Har intet med det at gøre</td>\n",
       "      <td>[ 0.49472684  0.52091426 -1.059791  ]</td>\n",
       "      <td>[0.44686255 0.45871928 0.09441815]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Det kan du sige, men med Artikel 13 bliver det...</td>\n",
       "      <td>[ 0.61225605  0.7817579  -0.8384904 ]</td>\n",
       "      <td>[0.4133752  0.4897316  0.09689319]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Tak gode mand! Da jeg fik notifikationen, troe...</td>\n",
       "      <td>[-1.0158063 -0.6964614  2.1499164]</td>\n",
       "      <td>[0.03834047 0.05276516 0.90889436]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Den er særdeles troværdig. Viser sandheden om ...</td>\n",
       "      <td>[ 1.1367214  -0.47244602 -0.9655315 ]</td>\n",
       "      <td>[0.7562952  0.15129997 0.09240479]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>MISS</td>\n",
       "      <td>Mere brug af GMO i landbruget kan ikke løse al...</td>\n",
       "      <td>[ 0.12570588  0.72970027 -1.148641  ]</td>\n",
       "      <td>[0.32164422 0.5884197  0.08993609]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Rusland fejrer femåret for annekteringen af Kr...</td>\n",
       "      <td>[-0.76456714  1.6279043  -0.502376  ]</td>\n",
       "      <td>[0.07552715 0.8263046  0.09816829]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Det er minimeret til det ekstreme, så måske ik...</td>\n",
       "      <td>[ 1.3416474 -0.5490633 -1.1396143]</td>\n",
       "      <td>[0.8099776  0.12227785 0.06774461]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>🇩🇰 #Superliga\\nGrupo 1 Descenso\\n29º Fecha\\n40...</td>\n",
       "      <td>[-1.4751852   1.7833322  -0.67671984]</td>\n",
       "      <td>[0.03420784 0.889778   0.07601419]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Så er der #klimakryds! Rigtig god valgkamp #dk...</td>\n",
       "      <td>[-1.406308    0.25475538  0.6032569 ]</td>\n",
       "      <td>[0.07285987 0.38360038 0.54353976]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Predicted Labels True Labels Misclassification  \\\n",
       "0           negative     neutral              MISS   \n",
       "1            neutral    negative              MISS   \n",
       "2            neutral    negative              MISS   \n",
       "3           positive    positive              TRUE   \n",
       "4           negative    negative              TRUE   \n",
       "..               ...         ...               ...   \n",
       "709          neutral    negative              MISS   \n",
       "710          neutral     neutral              TRUE   \n",
       "711         negative    negative              TRUE   \n",
       "712          neutral     neutral              TRUE   \n",
       "713         positive    positive              TRUE   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    Det er så ikke den, jeg var lidt for entusiast...   \n",
       "1                  Åh gud... Har intet med det at gøre   \n",
       "2    Det kan du sige, men med Artikel 13 bliver det...   \n",
       "3    Tak gode mand! Da jeg fik notifikationen, troe...   \n",
       "4    Den er særdeles troværdig. Viser sandheden om ...   \n",
       "..                                                 ...   \n",
       "709  Mere brug af GMO i landbruget kan ikke løse al...   \n",
       "710  Rusland fejrer femåret for annekteringen af Kr...   \n",
       "711  Det er minimeret til det ekstreme, så måske ik...   \n",
       "712  🇩🇰 #Superliga\\nGrupo 1 Descenso\\n29º Fecha\\n40...   \n",
       "713  Så er der #klimakryds! Rigtig god valgkamp #dk...   \n",
       "\n",
       "                              Logit Values                       Probabilities  \n",
       "0    [ 0.9340587  -0.01499593 -1.1753395 ]  [0.66294634 0.25663105 0.08042265]  \n",
       "1    [ 0.49472684  0.52091426 -1.059791  ]  [0.44686255 0.45871928 0.09441815]  \n",
       "2    [ 0.61225605  0.7817579  -0.8384904 ]  [0.4133752  0.4897316  0.09689319]  \n",
       "3       [-1.0158063 -0.6964614  2.1499164]  [0.03834047 0.05276516 0.90889436]  \n",
       "4    [ 1.1367214  -0.47244602 -0.9655315 ]  [0.7562952  0.15129997 0.09240479]  \n",
       "..                                     ...                                 ...  \n",
       "709  [ 0.12570588  0.72970027 -1.148641  ]  [0.32164422 0.5884197  0.08993609]  \n",
       "710  [-0.76456714  1.6279043  -0.502376  ]  [0.07552715 0.8263046  0.09816829]  \n",
       "711     [ 1.3416474 -0.5490633 -1.1396143]  [0.8099776  0.12227785 0.06774461]  \n",
       "712  [-1.4751852   1.7833322  -0.67671984]  [0.03420784 0.889778   0.07601419]  \n",
       "713  [-1.406308    0.25475538  0.6032569 ]  [0.07285987 0.38360038 0.54353976]  \n",
       "\n",
       "[714 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Predicted Labels': [\"negative\" if i == 0 else \"neutral\" if i == 1 else \"positive\" for i in preds_val_val],\n",
    "        'True Labels': [\"negative\" if i == 0 else \"neutral\" if i == 1 else \"positive\" for i in predictions_val.label_ids],\n",
    "        'Misclassification': [\"TRUE\" if preds_val_val[i] == predictions_val.label_ids[i] else 'MISS' for i, val in enumerate(preds_val_val)],\n",
    "        'Text': dataset_splitted_dict['valid']['text'],\n",
    "        'Logit Values': [str(i) for i in predictions_val.predictions],\n",
    "        'Probabilities': [str(i) for i in np.asarray(predictions_probabilities)]}\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.70      0.71       268\n",
      "     neutral       0.58      0.70      0.63       252\n",
      "    positive       0.73      0.58      0.65       194\n",
      "\n",
      "    accuracy                           0.67       714\n",
      "   macro avg       0.68      0.66      0.66       714\n",
      "weighted avg       0.68      0.67      0.67       714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Extract the true and predicted labels\n",
    "true_labels = df['True Labels']\n",
    "predicted_labels = df['Predicted Labels']\n",
    "\n",
    "# Create a mapping for the labels to numbers if needed\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "# Map the labels to numbers using the mapping\n",
    "true_labels_mapped = true_labels.map(label_mapping)\n",
    "predicted_labels_mapped = predicted_labels.map(label_mapping)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(true_labels_mapped, predicted_labels_mapped, target_names=label_mapping.keys())\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_NLP_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
