{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data path\n",
    "path = os.path.join(\"data\", \"tweets_data_temp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_prepare_dataset(file_path):\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(file_path)\n",
    "\n",
    "    # Remove all rows where language is not 'da'\n",
    "    dataset = dataset[dataset['language'] == 'da']\n",
    "\n",
    "    # Remove all columns except 'text' and 'label'\n",
    "    dataset = dataset[['text', 'label']]\n",
    "\n",
    "    # Remove all duplicates\n",
    "    dataset = dataset.drop_duplicates()\n",
    "\n",
    "    # Convert to dict and then to a Hugging Face Dataset\n",
    "    dataset = Dataset.from_dict(dataset)\n",
    "\n",
    "    print(\"Dataset loaded and prepared\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Split the dataset and convert into a Hugging Face DatasetDict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "def split_dataset(dataset, seed=42):\n",
    "    # 60% train, 20% validation, 20% test\n",
    "    train_test = dataset.train_test_split(test_size=0.4, seed=seed) \n",
    "    test_valid = train_test['test'].train_test_split(test_size=0.5, seed=seed)\n",
    "\n",
    "    # combine train, test and valid to one dictionary\n",
    "    dataset_splitted_dict = DatasetDict({\n",
    "        'train': train_test['train'],\n",
    "        'valid': test_valid['train'],\n",
    "        'test': test_valid['test']})\n",
    "    \n",
    "    print(\"Dataset splitted into train (60%), valid (20%) and test (20%)\")\n",
    "\n",
    "    # output the train dataset as a csv file\n",
    "    dataset_splitted_dict['train'].to_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "\n",
    "    return dataset_splitted_dict\n",
    "\n",
    "# Tokenize the dataset \n",
    "from transformers import AutoTokenizer\n",
    "from datasets import ClassLabel\n",
    "\n",
    "def tokenize_dataset(dataset, model_name=\"NbAiLab/nb-bert-large\", max_length=128):\n",
    "    # defining the labels\n",
    "    labels_cl = ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'])\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # defining a function to tokenize the text and translate all labels into integers instead of strings\n",
    "    def tokenize_function(example):\n",
    "        tokens = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        tokens['label'] = labels_cl.str2int(example['label'])\n",
    "        return tokens\n",
    "\n",
    "    # actually tokenizing the dataset\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset['train'].column_names) # batched=True speeds up tokenization by allowing to process multiple lines at once\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "# evaluation metrics\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric0 = evaluate.load(\"accuracy\")\n",
    "    metric1 = evaluate.load(\"precision\")\n",
    "    metric2 = evaluate.load(\"recall\")\n",
    "    metric3 = evaluate.load(\"f1\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric0.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "    f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing dataset...\n",
      "Dataset loaded and prepared\n",
      "Splitting dataset...\n",
      "Dataset splitted into train (60%), valid (20%) and test (20%)\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2143/2143 [00:00<00:00, 12322.54 examples/s]\n",
      "Map: 100%|██████████| 714/714 [00:00<00:00, 20472.47 examples/s]\n",
      "Map: 100%|██████████| 715/715 [00:00<00:00, 19011.48 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model (NbAiLab/nb-bert-large)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = load_and_prepare_dataset(path)\n",
    "\n",
    "print(\"Splitting dataset...\")\n",
    "dataset_splitted_dict = split_dataset(dataset)\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = tokenize_dataset(dataset_splitted_dict)\n",
    "\n",
    "print(\"Loading model (NbAiLab/nb-bert-large)...\")\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"NbAiLab/nb-bert-large\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16 # stating batch size\n",
    "epochs = 5\n",
    "learning_rate = 2e-4\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  num_train_epochs=epochs,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  warmup_steps=100,\n",
    "                                  weight_decay=0.01,\n",
    "                                  logging_dir=\"logs\",\n",
    "                                  logging_steps=10,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",  # Add this line\n",
    "                                  remove_unused_columns=False,\n",
    "                                  run_name=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 23/804 [19:00<10:45:37, 49.60s/it]\n",
      " 33%|███▎      | 268/804 [04:43<15:55,  1.78s/it]\n",
      "Downloading builder script: 100%|██████████| 7.55k/7.55k [00:00<00:00, 2.51MB/s]\n",
      "\n",
      "Downloading builder script: 100%|██████████| 7.36k/7.36k [00:00<00:00, 14.5MB/s]\n",
      "\n",
      "Downloading builder script: 100%|██████████| 6.77k/6.77k [00:00<00:00, 9.26MB/s]\n",
      "/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      " 33%|███▎      | 268/804 [05:20<15:55,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0856143236160278, 'eval_accuracy': 0.43216783216783217, 'eval_precision': 0.18676903516064355, 'eval_recall': 0.43216783216783217, 'eval_f1': 0.26082003933566433, 'eval_runtime': 37.7255, 'eval_samples_per_second': 18.953, 'eval_steps_per_second': 2.386, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 500/804 [09:21<05:08,  1.01s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0938, 'learning_rate': 1.890547263681592e-05, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 536/804 [10:03<04:28,  1.00s/it]/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      " 67%|██████▋   | 536/804 [10:31<04:28,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0793061256408691, 'eval_accuracy': 0.43216783216783217, 'eval_precision': 0.18676903516064355, 'eval_recall': 0.43216783216783217, 'eval_f1': 0.26082003933566433, 'eval_runtime': 28.2783, 'eval_samples_per_second': 25.284, 'eval_steps_per_second': 3.183, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 804/804 [15:06<00:00,  1.00s/it]/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      "100%|██████████| 804/804 [15:34<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0448379516601562, 'eval_accuracy': 0.43216783216783217, 'eval_precision': 0.18676903516064355, 'eval_recall': 0.43216783216783217, 'eval_f1': 0.26082003933566433, 'eval_runtime': 28.0049, 'eval_samples_per_second': 25.531, 'eval_steps_per_second': 3.214, 'epoch': 3.0}\n",
      "{'train_runtime': 934.1242, 'train_samples_per_second': 6.882, 'train_steps_per_second': 0.861, 'train_loss': 1.0959507956433652, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=804, training_loss=1.0959507956433652, metrics={'train_runtime': 934.1242, 'train_samples_per_second': 6.882, 'train_steps_per_second': 0.861, 'train_loss': 1.0959507956433652, 'epoch': 3.0})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:21<00:00,  4.66it/s]/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 90/90 [00:27<00:00,  3.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0448379516601562,\n",
       " 'eval_accuracy': 0.43216783216783217,\n",
       " 'eval_precision': 0.18676903516064355,\n",
       " 'eval_recall': 0.43216783216783217,\n",
       " 'eval_f1': 0.26082003933566433,\n",
       " 'eval_runtime': 45.8823,\n",
       " 'eval_samples_per_second': 15.583,\n",
       " 'eval_steps_per_second': 1.962,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:22<00:00,  4.93it/s]/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 90/90 [00:27<00:00,  3.22it/s]\n",
      " 99%|█████████▉| 89/90 [00:20<00:00,  4.26it/s]/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 90/90 [00:27<00:00,  3.33it/s]\n",
      "/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/wibe/Desktop/CogSci/NLP/NLP_exam_2023/venv_NLP_exam/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# get a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(trainer.predict(tokenized_dataset['valid']).label_ids, trainer.predict(tokenized_dataset['valid']).predictions.argmax(-1), target_names=['negative', 'neutral', 'positive'], output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score    support\n",
      "negative       0.375350  1.000000  0.545825  268.00000\n",
      "neutral        0.000000  0.000000  0.000000  252.00000\n",
      "positive       0.000000  0.000000  0.000000  194.00000\n",
      "accuracy       0.375350  0.375350  0.375350    0.37535\n",
      "macro avg      0.125117  0.333333  0.181942  714.00000\n",
      "weighted avg   0.140888  0.375350  0.204875  714.00000\n"
     ]
    }
   ],
   "source": [
    "# classification report as a dataframe\n",
    "print(pd.DataFrame(report).transpose())\n",
    "\n",
    "# save the report as a csv file\n",
    "pd.DataFrame(report).transpose().to_csv('classification_reports/classification_report_without_paraphrasings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_NLP_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
